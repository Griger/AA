---
title: "Trabajo3"
output: pdf_document
---
```{r, echo=FALSE}
library(MASS)
library(ISLR)
library(class)
library(e1071)
library(ROCR)
library(boot)
library(dismo)
library(glmnet)
library(randomForest)
library(gbm)
library(tree)
```

Una de las bases de datos con las que vamos a trabajar tiene "más pinta de cuadrática" entonces nos podemos plantear hacerlo del siguiente modo:

modelo3 <- lm(y ~ I(x2^2)+x2) tenemos que poner la palabra reservada I para que interprete correctamente la potencia.

poly() para hacer combinaciones polinómicas.

Puede ocurrir que haya sinergia entre atributos, es decir, que no sean independientes unos de otros, entonces vamos a ver cómo escrbimos una fórmula para que el modelo se ajuste como queremos a los datos que le pasamos.

modeloSinergico <- lm(y ~ x1*x2, data = datos) <=> lm(y ~ x1+x2+x1:x2, data = datos)

Despues de aprender un modelo podemos poner names(modelo) y nos dice los elementos que podemos consultar del modelo.

Lo convertimos a factor, asfactor

tune.knn va probando distintos parámetros y te devuelve el que mejor funciona.

#Ejercicio 1

## a) Usar las funciones de R pairs() y boxplot() para investigar la dependencia entre mpg y las otras características. ¿Cuáles de las otras características parece más útil para predecir mpg? Justificar la respuesta.

```{r}
set.seed(41192)
attach(Auto)
pairs(Auto)
pairs(mpg ~ displacement + horsepower + weight)
```

Como podemos ver las "gráficas de dependencias" de mpg con respecto a *displacement*, *horsepower* y *weight* son las gráficas que presentan un patrón más parecido entre ellas indicando que mpg tiene una relación fuerte con estas variables ya que se ajusta a ellas de un modo similar. Por ejemplo si vemos la gráfica con respecto a *acceleration* lo que tenemos es una nube de puntos mucho más dispersa. En cambio estas gráficas si que tienen un aspecto de ser ajustables linealmente.

```{r}
boxplot(mpg ~ cylinders)
boxplot(mpg ~ displacement)
boxplot(mpg ~ horsepower)
boxplot(mpg ~ weight)
boxplot(mpg ~ acceleration)
boxplot(mpg ~ year)
boxplot(mpg ~ origin)
boxplot(mpg ~ name)
```



## b) Seleccionar las variables predictoras que considere más relevantes.

Vamos a seleccionar aquellas que hemos visto en el apartado anterior que parecen seguir un patrón similar:

```{r}
datos = Auto[,c("mpg","displacement", "horsepower", "weight")]
```

## c) Particionar el conjunto de datos en un conjunto de entrenamiento (80%) y otro de test (20%). Justificar el procedimiento usado.

A priori había pensado en calcular en primer lugar la variable mpg1 del siguiente apartado para así poder realizar un particionamiento más homogéneo de las etiquetas positivas y negativas en los conjuntos de entrenamiento y test. El problema es que para hacer tal cosa tendría que calcular la mediana de todos los datos de Auto y lo que queremos es calcular la mediana, y por tanto el valor de mpg1 sólo en base a los datos de entrenamiento puesto que se dijo en teoría que para no contaminar el aprendizaje no podíamos usar los datos de test para calcular una mediana, sería como mirar los datos antes de aprender. Entonces he realizado simplemente un submuestreo de los datos aleatorio para dividirlos en entrenamiento y test.

```{r}
n = nrow(datos)
idx_train = sample(seq(n), ceiling(0.8*n))

datos.train = datos[idx_train,]
datos.test = datos[-idx_train,]
```
## d) Crear una variable binaria, mpg01, que será igual 1 si la variable mpg contiene un valor por encima de la mediana, y -1 si mpg contiene un valor por debajo de la mediana. La mediana se puede calcular usando la función median(). (Nota: puede resultar útil usar la función data.frames() para unir en un mismo conjunto de datos la nueva variable mpg01 y las otras variables Auto).

```{r}
mediana = median(datos.train$mpg)
mpg1.train = sapply(datos.train$mpg, function(x) if (x < mediana) return(0) else return(1))
mpg1.test = sapply(datos.test$mpg, function(x) if (x < mediana) return(0) else return(1))
```

* Ajustar un modelo de regresión logística a los datos de entrenamiento y predecir mpg01 usando las variables seleccionadas en b). ¿Cuál es el error de test del modelo? Justificar la respuesta.

```{r}
trainRL = data.frame(mpg01 = mpg1.train, datos.train)
testRL = data.frame(mpg01 = mpg1.test, datos.test)

RL = glm(mpg01 ~ displacement+horsepower+weight, data = trainRL, family = binomial)

prediccion = predict(RL, newdata = testRL, type = "response")
RL.pred = rep(0, length(testRL$mpg01))
RL.pred[prediccion > .5] = 1

cat("El % de errores en test con RL es: ", sum(testRL$mpg01 != RL.pred)/length(testRL$mpg01)*100)
```


* Ajustar un modelo K-NN a los datos de entrenamiento y predecir mpg01 usando solamente las variables seleccionadas en b). ¿Cuál es el error de test del modelo? ¿Cuál es el valor de K que mejor ajusta los datos?

```{r}
getErrorKNN <- function(datos.train, datos.test, et.train, et.test ){
  #normalizamos los datos
  train.norm = scale(datos.train[,c("weight","displacement","horsepower")])
  medias = attr(train.norm, "scaled:center")
  escalados = attr(train.norm, "scaled:scale")
  test.norm = scale(datos.test[,c("weight","displacement","horsepower")], medias, escalados)
  
  datos.full = rbind(train.norm, test.norm)
  et.full = as.factor(c(et.train,et.test))
  
  set.seed(75570417)
  mknns = tune.knn(datos.full, et.full, k=1:20, tunecontrol = tune.control(sampling = "cross"), cross = 10)
  
  mejor_k = mknns$best.model$k
  
  KNN = knn(train.norm, test.norm, et.train, k = mejor_k, prob = TRUE)
  cat("Se ha elegido como mejor k el: ", mejor_k, "\n")
  err.test = sum(et.test != KNN)/length(et.test)*100
  cat("El % de errores que obtenemos en el test es: ", err.test, "\n")
  #print(KNN)
  list(KNN, err.test)
}

KNN =  getErrorKNN(trainRL, testRL, mpg1.train, mpg1.test)[[1]]
```

* Pintar las curvas ROC (instalar paquete ROCR en R) y comparar y valorar los resultados obtenidos por ambos modelos.

```{r}
pRL = prediction(prediccion, testRL$mpg01)
perf = performance(pRL, "tpr", "fpr")
plot(perf, main = "Curvas ROC")

prob = attr(KNN, "prob")
prob = ifelse(KNN == "0", 1-prob, prob)
pKNN = prediction(prob, testRL$mpg01)
perf = performance(pKNN, "tpr", "fpr")


plot(perf,  add = TRUE, col = "red")
legend(0.8,0.2,c("RL", "KNN"), lty=c(1,1),lwd=c(2.5,2.5),col=c("black","red"))
```
Al inicio RL parece mas estable puesto que pasa más tiempo teniendo un tasa nula de falso positivos, ahora bien cuando esta tasa aumenta vemos como antes la misma pérdida o el mismo error de clasificación es el KNN el que acierta un mayor número de veces, es decir, que aunque cometemos el mismo error al menos clasificamos bien más muestras.

Cuando la tasa de falsos positivos aumenta es nuevamente la RL la que mejor se comporta puesto que tiene una tasa de verdaderos positivos más elevada aunqeue no difiere en gran medidad de la del KNN. Con lo cual en mi opinión es el KNN el que tiene un mejor comportamiento puesto que es el que da mejores resultados ante un error igual.

## e) (Bonus 1) Estimar el error de test de ambos modelos pero usando Validación Cruzada de 5-particiones. Comparar con los resulados obtenidos en el punto anterior.

```{r}
mediana = median(datos$mpg)
mpg01 = sapply(datos$mpg, function(x) if (x < mediana) return(0) else return(1))
fullData = data.frame(mpg01 = mpg01, datos)

RL = glm(mpg01 ~ displacement+horsepower+weight, data = fullData, family = binomial)
pp = cv.glm(fullData, RL, K = 5)

cat("El error de test obtenido haciendo VC en RL ha sido: ", pp$delta[1]*100, "\n")

folds = kfold(fullData, k = 5)
err = 0

for (i in seq(5)) {
  err = err + getErrorKNN(fullData[folds != i,], fullData[folds == i,], fullData[folds != i,]$mpg01, fullData[folds == i,]$mpg01)[[2]]
}

cat("El error de test obtenido haciendo VC en KNN ha sido: ", err/5)
```

## f) (Bonus 2) Ajustar el mejor modelo de regresión posible considerando la variables mpg como salida y el resto como predictoras. Justificar el modelo ajustando en base al patrón de los residuos. Estimar su error de entrenamiento y de test.


#Ejercicio 2 

##Usar la base de datos Boston (en el paquete MASS de R) para ajustar un modelo que prediga si dado un suburbio este tiene una tasa de criminalidad (crim) por encima o por debajo de la mediana. Para ello considere la variable crim como la variable salida y el resto como variables predictoras.

##a) Encontrar el subconjunto óptimo de variables predictoras a partir de un modelo de regresión-LASSO (usar paquete glmnet de R) donde seleccionamos sólo aquellas variables con coeficiente mayor a un umbral prefijado.

##b) Ajustar un modelo de regresión regularizada con "weight-decay" (ridge-regression) y las variables seleccionadas. Estimar el error residual del modelo y discutir si el conportamiento de los residuos muestran algún indicio de "underfitting".

Lo que vamos a hacer en primer lugar es emplear validación cruzada para obtener el mejor $s$ para nuestros datos, este s lo que hace es regular el impacto de la restricción de "contracción", la cual lleva a coeficientes más pequeños cuanto mayor sea su impacto, es decir, el valor de s.

Con el objetivo de tener un buen s lo que vamos a hacer es obtenerlo mediante validación cruzada en la cual emplearemos todos los datos, ya vimos en teoria que con validación cruzada si empleamos todos los datos podemos tener un buen estimador sin riesgo de que usar los datos de test suponga un problema de sobreajuste.

```{r}
attach(Boston)

set.seed(41192)
cv.out = cv.glmnet(as.matrix(Boston[,-1]), Boston[,1],alpha=1)
bestlam = cv.out$lambda.min #el mejor ese obtenido
```

Ahora aplicamos un modelo lasso a nuestros datos empleando el s que hemos obtenido, esto nos dará una serie de coeficientes para las distintas características de la muestra, aquellos que, en valor absoluto, estén por encima de un cierto umbral (que iremos modificando hasta obtener un buen resultado) serán los de las variables que seleccionemos para el siguiente apartado.

```{r}
out = glmnet(as.matrix(Boston[,-1]),Boston[,1], alpha = 1)
lasso.coef = predict(out, type="coefficients",s=bestlam)


umbral = 0.1
var_seleccionadas = which(abs(lasso.coef) > umbral)[-1]

#generamos un submuestreo aleatorio 80-20
set.seed(41192)
train = sample(1:nrow(Boston), nrow(Boston)*0.8)
test = (-train)

ridge.mod = glmnet(as.matrix(Boston[train, var_seleccionadas]), Boston[train,1], alpha = 0)
ridge.pred = predict(ridge.mod, s=bestlam,newx = as.matrix(Boston[test, var_seleccionadas]))

error = sqrt(mean((Boston[test,1] - ridge.pred)^2)/(nrow(Boston[test,])-2))
print(error)
```

El error que calculamos es el RSE que es la raíz cuadrada del error cuadrático medio, a continuación mostramos una tabla con los distintos parámetros que hemos probado. Los parámetros que hemos ido modificando han sido el umbral que marca la selección de las características y el s para el weight decay, ya que aunque tenemos uno óptimo para el Lasso no tiene por qué ser el mismo para esta nueva regresión, aunque veremo que sí. En la siguiente tabla recogemos los parámetros junto con el RSE obtenido con ellos (para la partición 80-20 que hacemos de los datos con la semilla seleccionada):

| umbral | s             | RSE        |
|--------|---------------|------------|
| 0.1    | bestlam       | 0.3129668  |
| 0.1    | 100           | 0.4107367  |
| 0.1    | 200000bestlam | 0.4997851  |
| 0      | bestlam       | 0.3214119  |
| 0.45   | bestlam       | 0.47722858 |


Como vemos el bestlam es el que mejores resultados da con el mismo umbral, también hemos obsevado que si dividimos el bestlam por prácticamente cualquier número el error no varía, no obstante y dado que veo que lo que marca el s es el peso de una condición que podríamos llamar de regularización opto por dejarlo lo "más grande posible", para evitar en la medida de lo posible sobreajuste. Cuando amplio demasiado el s, dándole mucho peso a la regularización entonces el algoritmo se olvida de ajustar los datos y da peores errores.

Por otro lado si para el umbral hacemos que se consideren todas las variables el error empeora aunque no demasiado, indicando que no hay tanto ruido que afecte al algoritmo como podríamos pensar. En cambio lo que sí que empeora más el resultado es ser demasiado elitistas con la selección de variables ya que según vemos estamos perdiendo información para un buen ajuste.

Por tanto en mi opinión observamos underfitting cuando le damos demasiado peso a la condición de regularización, cuando el s es demasiado grande, para los otros parámetros el error fuera de la muestra es relativamente bueno.

##c) Definir una nueva variable con valores -1 y 1 usando el valor de la mediana de la variable crim como umbral. Ajustar un modelo SVM que prediga la nueva variable definida (usar el paquete e1071 de R). Describir con detalle cada uno de los pasos dados en el aprendizaje del modelo SVM. Comience ajustando un modelo lineal y argumente si considera necesario usar algún núcleo. Valorar el resultado del uso de distintos núcleos.

Anteriormente ya hemos generado las muestras de entrenamiento y test, ahora vamos a tomar la mediana de los datos de entrenamiento (ya hemos dicho por qué esto se hace así) y luego a generar la variable que llamaremos, *crim1* a partir de ella:

```{r}
mediana = median(Boston[train,1])
crim1.train = sapply(Boston[train,1], function(x) if (x < mediana) return(-1) else return(1))
crim1.test = sapply(Boston[test,1], function(x) if (x < mediana) return(-1) else return(1))

#generamos un dataframe con estos datos + la nueva var.
data.train = data.frame(Boston[train,-1], crim1 = as.factor(crim1.train))
data.test = data.frame(Boston[test, -1], crim1 = crim1.test)
```

A continuación vamos a aplicar un modelo de support vector machine a los datos de entrenamienot, a priori el kernel que vamos a usar va a ser lineal. El principal problema que tenemos es que nuestros datos no tienen solo dor propiedades con lo cual el hecho de hacer un plot de la clasificación dada no nos dirá qué tipo de núcleo obtener. Lo que vamos a hacer es usasr la función *pair* para ver si encontramos algún tipo de patrón:

```{r}
data.full = rbind(data.train, data.test)
pairs(data.full)
```

Dado que es una clasificación binaria da la sensación de que el clasificador lineal va a ir, así que lo que vamos a hacer es entrenar un clasificador svm con cada uno de los núcleos disponibles y ver qué error fuera de la muestra obtenemos:
s
```{r}
svmfitL = svm(crim1~., data=data.train, kernel = "linear")
svm.pred = predict(svmfitL, newdata = data.test)
cat("Obtenemos un error de (lineal):", sum(svm.pred != data.test$crim1))
table(predict = svm.pred, truth = data.test$crim1)

svmfitS = svm(crim1~., data=data.train, kernel = "sigmoid")
svm.pred = predict(svmfitS, newdata = data.test)
cat("Obtenemos un error de (sigmoidal):", sum(svm.pred != data.test$crim1))
table(predict = svm.pred, truth = data.test$crim1)

svmfitR = svm(crim1~., data=data.train, kernel = "radial")
svm.pred = predict(svmfitR, newdata = data.test)
cat("Obtenemos un error de (radial):", sum(svm.pred != data.test$crim1))
table(predict = svm.pred, truth = data.test$crim1)

svmfitP = svm(crim1~., data=data.train, kernel = "polynomial")
svm.pred = predict(svmfitP, newdata = data.test)
cat("Obtenemos un error de (polinomial):", sum(svm.pred != data.test$crim1))
table(predict = svm.pred, truth = data.test$crim1)
```
Por lo tanto el que mejor resultados da es el clasificador con un núcleo lineal como sospechábamos.

Como vemos consideramos todas las variables excepto la variable crim ya que esta es la que queremos predecir, con lo que estaríamos contaminando el aprendizaje. Indicar que como comentó la profesora no tenemos que usar solamente aquellas variables seleccionadas en apartados anteriores del ejercicio, sino que consideramos todas. Esto se debe a que según qué modelo de aprendizaje estemos empleando unas variables tendrán más importancia que otras; cada modelo tiene sus criterios y "preferencias", entonces a priori, sin un análisis más exhaustivo de los datos, no podemos descartar ninguna de las características.

# Ejercicio 3

##Usar el conjunto de datos Boston y las librerías randomForest y gbm de R.

##1. Dividir la base de datos en dos conjuntos de entrenamiento (80%) y test (20%).

```{r}
set.seed(41192)
attach(Boston)
train = sample(nrow(Boston), nrow(Boston)*0.8)
test = -train
```

##2. Usando la variable medv como salida y el resto como predictoras, ajustar un modelo de regresión usando bagging. Explicar cada uno de los parémtros usados. Calcular el error de test.

```{r}
set.seed(41192)
bag = randomForest(medv ~., data = Boston, subset = train, mtray = ncol(Boston)-1, importance = TRUE)
pred = predict(bag, newdata = Boston[test,])
cat("El MSE con bagging es: ", mean((pred - Boston[test,]$medv)^2), "\n")
```

##3. Ajustar un modelo de regresión usando Random Forest. Obtener una estimación del número de árbol necesario. Justificar el resto de parámetros usados en el ajuste. Calcular el error de test y compararlo con el obtenido con bagging.

Para obtener una aproximación del número de árboles necesario lo que vamos a hacer es usar la validación cruzada con 5 particiones probando valores de 100 a 500 dando saltos de 20 en 20.
```{r}
set.seed(41192)
randomfcv = rfcv(Boston[,1:13], Boston$medv)
#entonces voy a usar mtray = 6 orque es el que menos MSE da quitando el 13 que sería hacer el bagging
#vamos a obtener una estimación del número de árboles necesarios
CVerrs = sapply(seq(100, 500, 20), function(x) {
  print(x)
  K = 5
  folds = kfold(Boston, k = K)
  err = 0

  for (i in seq(K)) {
    rf = randomForest(medv ~ ., data = Boston, subset = which(folds != i), mtry = 6, importance = TRUE)
    pred = predict(rf, newdata = Boston[folds == i,])
    err = err + mean((pred - Boston[folds == i,]$medv)^2)
  }
  
  err/K
})

opt_ntree = seq(100,500,20)[which.min(CVerrs)]
#uso 6 porque es el que menos MSE da quitando el 13 que seria hacer el bagging
rf = randomForest(medv ~., data = Boston, subset = train, mtray = 6, importance = TRUE)

pred = predict(rf, newdata = Boston[test,])
cat("El MSE con rf con 500 árboles es: ", mean((pred - Boston[test,]$medv)^2), "\n")

rf = randomForest(medv ~., data = Boston, subset = train, mtray = 6, ntree = opt_ntree, importance = TRUE)

pred = predict(rf, newdata = Boston[test,])
cat("El MSE con rf con opt árboles es: ", mean((pred - Boston[test,]$medv)^2), "\n")
```

Por lo tanto como podemos comprobar el error con random forest es algo mayor que con bagging pero no demasiado, con lo cual podemos pensar en dos cosas: lo primero es que quizás todas las variables no sean relevante para la clasficación e incluso que introduzcan ruido en ésta, en segundo lugar vemos como el random forest con un procedimiento que será más eficiente, al emplear menos variables, obtiene un mejor resultado. Por lo tanto se pone de relieve como una buena seleccion de variables es muy importante. Ademas ha habido una ocasion en la cual el error con random forest ha sido incluso menor, pero ahora con esta semilla los resultados son estos.

##4. Ajustar un modelo de regresión usando Boosting (usar gbm con distribution = 'gaussian'). Calcular el error de test y compararlo con el obtenido con bagging y Random Forest.

```{r}
set.seed(41192)
BOSton = gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", n.trees = 500)
pred = predict(BOSton, newdata = Boston[test,], n.trees = 500)
cat("El MSE con boosting es: ", mean((pred - Boston[test,]$medv)^2), "\n")
```

#Ejercicio 4

##Usar el conjunto de datos OJ que es parte del paquete ISLR.

##1. Crear un conjunto de entrenamiento conteniendo una muestra aleatoria de 800 observaciones, y un conjunto de test conteniendo el resto de observaciones. Ajustar un árbol a los datos de entrenamiento, con "Purchase" como la variable respuesta y las otras variables como predictores (paquete tree de R).

```{r}
set.seed(41192)
attach(OJ)
train = sample(nrow(OJ), 800)
test = -train

tree.OJ = tree(Purchase ~ ., OJ[train, ])
```

##2. Usar la función summary() para generar un resumen estadístico acerca del árbol y describir los resultados obtenidos: tasa de error de "training", número de nodos del árbol, etc.

```{r}
summary(tree.OJ)
```

Lo primero que observamos es que de las 17 variables con las que podría trabajar el árbol para realizar la clasificacion realmente solo usa 3: *LoyalCH*, *PriceDiff* y *ListPriceDiff*. Por lo tanto o bien estas características no aportan ninguna información relevante o la información que aportan estas tres es suficiente.

Tenemos que hay 8 nodos terminales por lo tanto en base a las variables anteriormente mencionadas te hacen 8 particiones de los datos a partir de las cuales podemos saber cuál de los dos valores posibles de *Purchase* toma cada objeto de los datos de training.

Tenemos que se clasifican mal 116 elementos de los 800 por lo tanto aunque el algoritmo ha considerado que no necesita dividir más los datos, quizás porque sus medidas le han dicho que ya no va a obtener nueva información, todos los datos no han sido bien clasificados; hay datos que han "caído" en particiones donde la clase mayoritaria de *Purchase* no era la suya.

La desviación media se obtiene dividiendo por el número de total de muestras menos el número de nodos final la siguiente cantidadad:

$-2\sum_m\sum_k n_{mk}log\hat{p}_{mk}$

donde $n_{mk}$ es el número de muestras de la clase k que hay en el nodo terminal m-ésimo y $\hat{p}_{mk}$ es la proporción de muestras de la clase k que hay en el nodo m-ésimo, por lo tanto cuanot más nodos de esa clase haya, mayor será esta proporción y en consecuencia el logaritmo será mayor aportando menos a la sumatoria del error, que es lo que estamos midiendo en definitiva.

##3. Crear un dibujo del árbol e interpretar los resultados.

```{r}
plot(tree.OJ)
text(tree.OJ, pretty = 0)
```

Entonces lo que vemos que primero se clasifican los datos en base a si el valor de *LoyalCH* está por debajo o por encima de 0.5036, luego si está pode debajo esas muestras se dividen nuevamente en base a si el valor de LoyalHC está por debajo de 0.276142 o no y en caso afirmativo volvemos a dividir por un umbral, aunque como vemos esta división resulta inútil pues ambas particiones son etiquetadas por **MM**. En caso de no estar por debajo de 0.276142 entonces entra en juego la variable PriceDiff que sí que establece dos particiones con etiquetas distintas.

En la otra rama del árbol cuando el valor de *LoyalHC* está por encima de 0.5036 entonces si está por encima de 0.738331 se etiquetan los datos como **CH** y en caso contrario se parten los datos en base a un umbral con *PriceDiff* y en caso de estar por encima de dicho umbral es la variable *ListPriceDiff*, la que quedaba por usarse de las que hemos dicho antes que se para los datos en base a un umbral etiquetando cada una de las dos particiones con una clase distinta.

##4. Predecir la respuesta de los datos de test, y generar e interpretar la matriz de confusión de los datos de test. ¿Cuál es la tasa de error de test? ¿Cuál es la precisión del test?

```{r}
predOJ = predict(tree.OJ, OJ[test, ], type="class")
table(predOJ, OJ[test,]$Purchase)
```
La tasa de error de test es $\dfrac{40+20}{270} = 0.222222$.

La precisión es $\dfrac{137}{137+40} = 0.774011$

##5. Aplicar la función cv.tree() al conjunto de "training" y determinar el tamaño óptimo del árbol. ¿Qué hace cv.tree?

La función cv.tree lo que haces es una validación cruzada particionando el conjunto de datos que le demos en tantas particiones como indiquemos, por defecto es 10. Enotnces a partir de esta validación cruzada podemos estimar el valor óptimo de algunos parámetros del algoritmo para luego aplicarlos al conjunto de datos.

```{r}
cvTree = cv.tree(tree.OJ, FUN = prune.misclass)
print(cvTree)
```

Con lo cual en base a la tasa de error de clasificacion el tamaño optimo es el 4, nodos terminales, recordemos que nosotros obteníamos 8 nodos terminales, con lo cuál aquí se reduce a la mitad.

##Bonus 4. Generar un gráfico con el tamaño del árbol en el eje x (número de nodos) y la tasa de error de validación cruzada en el eje y. ¿Qué tamaño de árbol corresponde a la tasa más pequeña de error de clasificación por validación cruzada?

```{r}
#plot(cvTree)
plot(cvTree$size, cvTree$dev, type = 'b')
```


---
title: \textbf{Proyecto Final. Cardiotocography}
author: "Anabel Gómez Ríos y Gustavo Rivas Gervilla"
date: "16 de junio de 2016"
output: pdf_document
---

```{r, include=FALSE}
library(caret) # para qué?
library(glmnet)
library(fields)
```


#1. Definición del problema a resolver y enfoque elegido.

En este proyecto vamos a trabajar con una base da datos algo mayor que las que hemos venido usando en las prácticas (2126 instancias con 23 atributos cada una) con el objetivo de poner en práctica los conocimientos adquiridos en la asignatura para resolver un problema de clasificación del mundo real.
  
La base de datos elegida es Cardiotocography del respositorio de bases de datos UCI la cual la podemos descargar \href{https://archive.ics.uci.edu/ml/datasets/Cardiotocography#}{\textbf{aquí}}. En esta base de datos se recogen distintas características de cardiotrogafías en las cuales se mide la frencuencia cardiaca fetal (FHR), los movimiento fetales (FM) y las contracciones uterinas (UC), obteniendo las siguientes características a partir de estos datos:

\begin{enumerate}
\item LB: punto de referencia del FHR en pulsaciones por minuto.
\item AC: aceleraciones del pulso por segundo.
\item FM: movimientos fetales por segundo.
\item UC: contracciones uterinas por segundo.
\item DL: deceleraciones suaves por segundo.
\item DS: deceleraciones fuertes por segundo.
\item DP: deceleraciones prolongadas por segundo.
\item ASTV: porcentaje de tiempo con variaciones anormales cortas del pulso.
\item MSTV: media de las variaciones anormales cortas del pulso.
\item ALTV: porcentaje de tiempo con variaciones anormales largas del pulso.
\item MLTV: media de las variaciones anormales largas del pulso.
\item Width: amplitud del histograma FHR.
\item Min: mínimo del histograma FHR.
\item Max: máximo del hisotograma FHR.
\item Nmax: número de picos en el histograma.
\item Nzeros: número de ceros en el histograma.
\item Mode: moda del histograma.
\item Mean: media del histograma.
\item Median: mediana del histograma.
\item Variance: varianza del histograma.
\item Tendency: tendencia del histograma.
\item CLASS: código del tipo de patrón del histograma FHR [1-10].
\item NSP: código del estado fetal. [1: Normal, 2: Sospechoso y 3: Patológico]
\end{enumerate}

Lo que queremos es emplear estos datos para poder predecir ante una nueva cardiotocografía si el estado del feto es normal, sospecho o patológico, es decir, vamos a predecir la variable NSP con el resto. Además, vamos a hacer la clasificación también según la variable CLASS, puesto que también es una de las "preguntas" en la base de datos.  

El enfoque elegido por tanto es hacer clasificación multiclase para clasificar nuevos datos según dos variables (por separado), una que tiene 3 clases y otra que tiene 10.

```{r}
datos <- read.csv("datos.csv")
```

#2. Codificación de los datos de entranda para hacerlos útiles a los algoritmos.

Nuestra base de datos estaba contenida en una hoja de cálculo. Para poder usarla dentro de R lo que hemos hecho es generar un CSV con los datos previamente formateados puesto que hemos tenido que cambiar el formato decimal de algunas columnas para que fuese el que emplea R. Además en el fichero original aparecían más variables como la fehca y el tiempo de inicio y fin de la cardiotocografía las cuales no hemos considerado relevantes para el estudio por lo que no están presententes en el CSV.

#3. Valoración del interés de las variables para el problema y selección de un subconjunto en su caso.

En primer lugar tenemos que `Width` se calcula como la diferencia entre `Max` y `Min` con lo cual suponemos que una de las tres no tendrán relevancia ya que la información aportada por ella se puede deducir de las otras dos.  

Para el resto de variables dado el poco conocimiento que tenemos en la materia no podemos saber qué factores son los que más influyen en determinar el estado del feto por tanto hemos decidido realizar un análisis de componentes principales para ver si podemos reducir el número de variables a considerar, haciendo que los algoritmos sean mÃ¡s eficientes en tiempo. La técnica que hemos usado en clase para tal propósito ha sido emplear el Lasso para obtener aquellas variables que sus coeficientes estuviesen por encima de un cierto umbral determinado por nosotros. Esto precisamente es lo que nos ha llevado a decantarnos por el PCA ya que con él podemos saber el conjunto de variables que son capaces de explicar al menos 95% de la variabilidad de los datos (aunque podemos cambiar este 95% y aumentarlo para que sea más estricto). Para saber cómo emplear PCA en R hemos consultado el enlace [2] de la bibliografía.  

Lo primero que vamos a hacer es separar los datos en las muestras de entrenamiento y test (80-20) que emplearemos a lo largo de todo el estudio. En esta ocasión como la variable a predecir no depende de la media de otras variables entonces vamos a poder realizar un particionado homogéneo de los datos para tener una distribución de las clases de cada muestra lo más uniforme posible (no corremos el riesgo de contaminar la variable con datos de test como un ocurría en prácticas).  

```{r}
set.seed(1)
# Cogemos los índices del 80% de los datos para cada clase
train_idx <- c(sample(which(datos$NSP == 1), size =
                        ceiling(0.8*sum(datos$NSP==1))),
               sample(which(datos$NSP == 2), size =
                        ceiling(0.8*sum(datos$NSP==2))),
               sample(which(datos$NSP == 3), size =
                        ceiling(0.8*sum(datos$NSP==3))))

# Hacemos el conjunto de train con estos índices
train <- datos[train_idx,]
# Hacemos el conjunto de test con todas aquellas variables que no tengan estos
# índices
test <- datos[-train_idx,]
```

```{r}
set.seed(1)
# Cogemos los índices del 80% de los datos para cada clase

train_idx <- c(sample(which(datos$CLASS == 1), size =
                        ceiling(0.8*sum(datos$CLASS==1))),
               sample(which(datos$CLASS == 2), size =
                        ceiling(0.8*sum(datos$CLASS==2))),
               sample(which(datos$CLASS == 3), size =
                        ceiling(0.8*sum(datos$CLASS==3))),
               sample(which(datos$CLASS == 4), size =
                        ceiling(0.8*sum(datos$CLASS==4))),
               sample(which(datos$CLASS == 5), size =
                        ceiling(0.8*sum(datos$CLASS==5))),
               sample(which(datos$CLASS == 6), size =
                        ceiling(0.8*sum(datos$CLASS==6))),
               sample(which(datos$CLASS == 7), size =
                        ceiling(0.8*sum(datos$CLASS==7))),
               sample(which(datos$CLASS == 8), size =
                        ceiling(0.8*sum(datos$CLASS==8))),
               sample(which(datos$CLASS == 9), size =
                        ceiling(0.8*sum(datos$CLASS==9))),
               sample(which(datos$CLASS == 10), size =
                        ceiling(0.8*sum(datos$CLASS==10))))

#train_idx <- sample(seq(1, nrow(datos)), ceiling(0.8*nrow(datos)))
# Hacemos el conjunto de train con estos índices
train10 <- datos[train_idx,]
# Hacemos el conjunto de test con todas aquellas variables que no tengan estos
# índices
test10 <- datos[-train_idx,]
```


Ahora vamos a quitar las variables `NSP` y `CLASS` de `train` y test, ya que son las salidas, y las vamos a guardar en dos vectores aparte.

```{r}
NSP.train <- train$NSP
train <- train[,-c(22,23)]
NSP.test <- test$NSP
test <- test[,-c(22,23)]

CLASS.train <- train10$CLASS
train10 <- train10[,-c(22,23)]
CLASS.test <- test10$CLASS
test10 <- test10[,-c(22,23)]
```


Vamos a hacer un `summary` sobre los datos de `train` para ver si podemos descartar alguna variable que a simple vista se vea que no va a aportar nada.

```{r}
summary(train)
summary(train10)
```

Como vemos, la variable DS tiene máximo y mínimo 0, con lo que es igual a 0 para todas las variables y por tanto no van a infuir para nuestro análisis en el conjunto de train. Lo que hacemos por tanto es quitarla de dicho conjunto.

```{r}
# Quitamos la variable DS, que ocupa la sexta columna
train <- train[,-6]
test <- test[,-6] # ESTO NO SABEMOS SI SE PUEDE HACER
train10 <- train10[,-6]
test10 <- test10[,-6]
```

Como hemos podido ver hay muchas que tienen valores cercanos a cero, pero sobre estos no podemos decir nada en claro, así que vamos a pasar a utilizar el algoritmo PCA. Para ello vamos a utilizar la función `prcomp` del paquete `stats` instalado por defecto en `R`.

El análisis de componente principales (PCA) sigue la siguiente idea: nosotros podemos tener nuestras muestran con gran multitud de características, es decir, muestras con una elevada dimensión. Ahora bien pueden darse caso en que no todas estas características tengan la misma relevancia, no aporten la misma información. Por motivos de eficiencia computacional y obtener un modelo más sencillo es claro que es bueno reducir este número de variables, y esto es lo que hace el PCA. Buscamos una representación de los datos de dimensión más baja que capture toda (o una cantidad considerable) de la información. Un dimensión es interesante en términos de cómo las observaciones varían a lo largo de dicha dimensión.

Si nuestras muestras tienen el siguiente conjunto de características $X_1, X_2, ..., X_p$ entonces primer componente principal será $Z_1 = \phi_1X_1 + ... + \phi_pX_p$ y buscamos entoncecs una combinación lineal de las características anteriores, cumpliendo que el cuadrado de los coeficientes (*loadings*) sumen uno, que maximize la varianza, es decir, que maximice $\frac{1}{n}\sum_{i = 1}^n \left( \sum_{j = 1}^p \phi_{j1}x_{ij} \right)^2$ (estamos sumoniendo que las variables tienen media cero para estos cálculos). Señalar que la condición de normalización que le imponemos a los *loadings* es para que no ganemos máxima varianza simplemente haciéndolos crecer mucho. Para calcular el resto de componente principales se hace de la misma manera restringiendo además a que no estén correlados con los anteriores; que sean ortogonales (si vemos los *loadings* como vectores) a los anteriores.

Ya hemos asumido antes que la media de cada variable es cero ya que sólo estamos interesados en la varianza y esto facilita los cálculos. Por otro lado, si no tenemos ninguna restricción que lo impida, es conveniente escalar las variables de modo que no tengan una más influencia que otra simplemente por la escala en la que está medida, por ejemplo si una variable da saltos de 1000 en 1000 y otra de 10 en 10, aunque los datos de la primera presenten menos varianza debido a la magnidtud de los datos ésta tendrá más influencia.

```{r}
pca.out <- prcomp(train, center = TRUE, scale = TRUE)
pca.out10 <- prcomp(train10, center = TRUE, scale = TRUE)
```

VENDER HUMO

```{r}
biplot(pca.out, scale = 0)
plot(pca.out, main="PCA")
summary(pca.out)
```

Como podemos ver con `summary()`, con las 14 primeras componentes principales estamos explicando un 96% de los datos, y son con las que nos vamos a quedar para hacer el estudio reducido y ver si hay mejora al utilizar PCA. Vamos a hacer entonces la combinación lineal que nos da PCA para obtener el nuevo conjunto de train:

```{r}
trainPCA <- apply(pca.out$rotation, 2, function(x) {
  apply(train, 1, function(y) {
    x%*%y
  })
})

trainPCA10 <- apply(pca.out10$rotation, 2, function(x) {
  apply(train10, 1, function(y) {
    x%*%y
  })
})
```

Vamos a hacerle la combinación lineal al conjunto de test también con las componentes principales de train:

```{r}
testPCA <- apply(pca.out$rotation, 2, function(x) {
  apply(test, 1, function(y) {
    x%*%y
  })
})


testPCA10 <- apply(pca.out10$rotation, 2, function(x) {
  apply(test10, 1, function(y) {
    x%*%y
  })
})
```

#4. Normalización de las variables (en su caso).

En el caso de modelos de aprendizaje que trabajan por similaridad, es decir, por distancia entre las muestras para asignar una clase a un dato (como son el KNN y las funciones de base radial) es conveniente normalizar las características de modo que evitemos que unas tengan más peso que otras en las decisiones del modelo, debido a las diferencias de magnitud.

Entonces vamos a normalizar las variables antes de aplicar ningún modelo de aprendizaje para así trabajar con los mismos datos. Entonces vamos a proceder a normalizar los datos, para ello igual que hemos hecho en prácticas anteriores vamos a normalizar los datos de train y empleando los factores de normalización de dicho proceso normalizaremos los de test. De este modo no vamos a contaminar los datos de entrenamiento con información sobre los de test, asegurando que el proceso de aprendizaje se adecuado.

```{r}
train <- scale(train)
medias <- attr(train, "scaled:center")
escalados <- attr(train, "scaled:scale")
test <- scale(test, medias, escalados)

trainPCA <- scale(trainPCA)
medias <- attr(trainPCA, "scaled:center")
escalados <- attr(trainPCA, "scaled:scale")
testPCA <- scale(testPCA, medias, escalados)
```

```{r}
train10 <- scale(train10)
medias <- attr(train10, "scaled:center")
escalados <- attr(train10, "scaled:scale")
test10 <- scale(test10, medias, escalados)

trainPCA10 <- scale(trainPCA10)
medias <- attr(trainPCA10, "scaled:center")
escalados <- attr(trainPCA10, "scaled:scale")
testPCA10 <- scale(testPCA10, medias, escalados)
```


#5. Selección de las técnicas y valoración de la idoneidad de las mismas frente a otras alternativas.

Es claro que no podemos plantearnos usar el perceptron ya que de inicio estamos antes un problema de clasifación no binaria con lo que no vamos a poder realizar una buena clasificación por medio de él.

De entre los modelos paramétricos que hemos visto en la asignatura nos vamos a decantar por la regresión logística multinomial. Podríamos haber considerado el SVM de núcleo lineal pero no conocemos la naturaleza de los datos, con lo cual no tenemos seguridad de vayan a ser linealmente separables, entonces preferimos considerar un modelo el cual no dependa de la disposición espacial de los datos como es la regresión logística.

Por otro lado las funciones de base radial paramétricas no nos parecen una buena opción ya que dependen fuertemente de cómo estén distribuidos los datos, es decir, les ocurre como al KNN; si tenemos puntos próximos al punto a etiquetar de clase distinta a la real del punto entonces la clasificación no será buena. En cambio pensamos que dado que la regresión logística nos da una visión probabilística del etiquetado será más robusta a estas situaciones.

A continuación vamos a explicar cómo funciona la regresion logística multinomial:  




#6. Aplicación de las técnicas especificando claramente qué algoritmos se usan en la estimación de los parámetros, los hiperparámetros y el error de generalización.

Como podemos leer en el libro ISLR regresión logística y SVM tienen funciones de pérdida muy parecidas, en consecuencia los rendimientos que ofrecen son muy parecidos. Ahora bien lo que distingue a ambos en su comportamiento según el solapamiento que haya entre las distintas clases de la muestra. Así cuando tenemos un solamiento mayor la regresión logística muestra, experimentalmente, un mejor rendimiento que el SVM. En cambio cuando los datos son separables es el SVM el que se comporta mejor.

Podríamos pensar en que SVM cuenta con la potencia que le aportan los núcleos, no obstante si el solapamiento entre las clases es considerable ningún núcleo será el adecuado, es decir, no tendríamos seguridad de que SVM funcione mejor.

Por tanto para decidirnos vamos a medir el solapamiento entre las distintas clases, esto lo haremos mediante la métrica CSM que se basa en realizar un cociente entre las distancias de los puntos de cada clase al punto medio de su clase con las distancias entre los puntos medios de las clases al punto medio de la muestra total. Así si las primeras son muy grandes el índice, J (que es este cociente), será pequeño indicando que hay solapamiento entre las clases, ya que por decirlo de alguna manera las clases están más dispersas que la muestra, indicando que efectivamente han de estar entremezcladas. Haciendo varios experimentos nos hemos dado cuenta de que si las clases están separadas pero juntas este índice el 0.5. Si las clases están separadas y además hay separación entre ellas J será mayor que 0.5 (y mayor cuanta más separación haya) y si las clases están solapadas J será menor que 0.5. Este estudio lo hemos sacado gracias a los enlaces [4] y [5].

A continuación mostramos el código de la función que nos medirá este índice de solapamiento para una muestra dada:

```{r}
classSepMeasure <- function(trainp, clases, numClases) {
  train <- as.matrix(trainp)
  m <- apply(train, 2, sum)/nrow(train)
  mis <- sapply(1:numClases, function(i) {
    datosi <- train[clases==i, ]
    apply(datosi, 2, sum)/nrow(datosi)
  }) 
  n <- 20
  Sw <- matrix(0, n, n)
  for (i in 1:numClases) {
    matriz <- matrix(0, n, n)
    datosi <- train[clases==i, ]
    for(j in 1:nrow(datosi)) {
      matriz_j <- (datosi[j,] - mis[,i])%*%t(datosi[j,] - mis[,i])
      matriz <- matriz + matriz_j
    }
    Sw <- Sw + matriz
  }
  
  Sb <- matrix(0, n, n)
  for (i in 1:numClases) {
    ni <- sum(clases == i)
    Mi <- ni*((mis[,i] - m)%*%t(mis[,i] - m))
    Sb <- Sb + Mi
  }
  
  J <- sum(diag(Sb))/sum(diag(Sw))
  
  return(J)
}
```

```{r}
classSepMeasure(train10, CLASS.train, 10)
classSepMeasure(trainPCA10, CLASS.train, 10)
classSepMeasure(train, NSP.train, 3)
classSepMeasure(trainPCA, NSP.train, 3)
```

Como vemos, este índice es menor que 0.5 en todos los casos, pero es que además cuando tenemos en cuenta 3 clases en lugar de 10 (`NSP`) este índice es un poco mayor que 0.1, con lo que las clases están muy solapadas y por tanto nos vamos a decantar por la regresión logística en ambos casos, tanto con 10 clases como con 3.

Empezamos haciendo el análisis para las 3 clases, es decir, según la variable `NSP`, y además empezamos haciéndolo con la transformación que nos ha dado PCA.

Para ello, vamos a utilizar regresión logística con regularización weigth-decay con el paquete `glmnet`. Como sabemos, hay un coeficiente, $\lambda$, que regula la influencia que tiene la condición de regularización (de "contracción") en el ajuste. Para elegir dicho $\lambda$ vamos a hacer validación cruzada con el mismo paquete haciendo uso de la función `cv.glmnet`, que nos devuelve justo el mejor $\lambda$ encontrado entre una rejilla. Esta validación cruzada la vamos a hacer con cinco particiones especificándolo con la variable `nfolds = 5` indicando que la regularización que queremos hacer es weight-decay (que se especifica con el parámetro `alpha = 0`). El porqué hemos elegido esta regularización está explicado en la sección dedicada para ello, la número 7. Además, como tenemos 3 clases y la clasificación no es binaria vamos a especificar con el parámetro `family = "multinomial"` que queremos hacer clasificación multiclase. Vamos a hacer que elija como $\lambda$ aquel que minimice la media de los errores cuadráticos, para lo que tenemos que poner como argumento a la función `type.measure = "mse"`.

```{r}
# Fijamos primero la semilla ya que la partición para cross validation es 
# aleatoria.
set.seed(1)
cv.fit <- cv.glmnet(as.matrix(trainPCA), NSP.train, alpha = 0, 
                    family = "multinomial", nfolds = 5, type.measure = "mse")
lambda <- cv.fit$lambda.min
print(lambda)
```

Vamos a pintar, para todos los valores de $\lambda$ que ha tomado `cv.glmnet()`, el logaritmo de dichos valores en el eje X y el error cuadrático medio para ese valor de $\lambda$ en el eje Y:

```{r}
plot(cv.fit)
```

Como vemos, el logaritmo del valor de $\lambda$ que nos ha salido, `r log(lambda)`, coincide con el mínimo valor de los errores cuadráticos medios.


EXPLICAR QUÉ HACE GLMNET


```{r}

modeloPCA3 <- glmnet(as.matrix(trainPCA), NSP.train, alpha = 0, 
                     family = "multinomial", lambda = lambda)
predicciones <- predict(modeloPCA3, s=lambda, newx = testPCA, type="response")
```

Esto devuelve para cada nuevo dato la probabilidad de que pertenezca a cada clase. Lo que vamos a hacer ahora es quedarnos con aquella clase a la que corresponda la máxima probabilidad y devolver dicha clase.

```{r}
data <- as.matrix(data.frame(predicciones))
clases.predichas <- apply(data, 1, function(x) {
  which.max(x)
})
cat("Porcentaje de acierto sobre 3 clases con PCA:", 
    100*sum(clases.predichas == NSP.test)/length(NSP.test))
```

Vamos a hacerlo ahora sin PCA a ver la diferencia que hay con PCA, a ver si merece la pena bajar las dimensiones porque no se pierde mucha información o por el contrario no merece la pena porque sale una predicción mucho mejor con todas las variables (en nuestro caso que tampoco tenemos muchas variables y podemos permitirnos hacer este estudio).

```{r}
# Fijamos primero la semilla ya que la partición para cross validation es 
# aleatoria.
set.seed(1)
cv.fit <- cv.glmnet(as.matrix(train), NSP.train, alpha = 0, 
                    family = "multinomial", nfolds = 5, type.measure = "mse")
lambda <- cv.fit$lambda.min
modelo3 <- glmnet(as.matrix(train), NSP.train, alpha = 0, 
                     family = "multinomial", lambda = lambda)
predicciones <- predict(modelo3, s=lambda, newx = test, type="response")

data <- as.matrix(data.frame(predicciones))
clases.predichas <- apply(data, 1, function(x) {
  which.max(x)
})
cat("Porcentaje de acierto sobre 3 clases sin PCA:", 
    100*sum(clases.predichas == NSP.test)/length(NSP.test))

```

Como vemos el porcentaje de acierto es similar: es un 1% mayor. Sin embargo, en el problema que nos ocupa, quizás sí sea una diferencia significativa. Aunque en realidad teniendo en cuenta el porcentaje de acierto, que tampoco es muy alto, quizás habría que centrarse primero en mejorarlo cambiando de técnica (mejorando con técnicas no paramétricas como vamos a hacer a continuación) que preocuparse por ese 1%. TODO GORDO.

Vamos ahora a enfocar el problema desde las 10 clases. En este caso hemos tenido problemas con el paquete `glmnet` ya que nos devolvía siempre la misma clase. No sabíamos si esto era así porque no había relación entre las etiquetas y los datos realmente o porque el paquete no funcionaba bien con tantas clases o en este problema en concreto por causas ajenas a nosotros, por lo que hemos hecho un experimento a ver si tratándolo como un problema binario del tipo *uno contra todos* (OVA) nos devuelve salidas lógicas. Lo que hemos hecho ha sido, para cada clase, hacer un problema binario cogiendo todas las instancias de train de esa clase y una muestra del mismo tamaño de las demás, y pasarle después el resto de muestras de train como datos de test, esperando que para todas ellas devolviera que no pertenecen a la clase considerada:

```{r}
for (i in 1:10) {
  set.seed(1)
  clase = i
  # Tomamos los datos de la clase i
  train_una_clase = train10[CLASS.train == clase,]
  n_datos_clase = nrow(train_una_clase)
  # Tomamos una muestra del resto del mismo tamaño
  train_idx <- sample(which(CLASS.train != clase), n_datos_clase)
  train.resto = train10[train_idx,]
  # Juntamos la clase considerada con la muestra de las demás
  train.OVA = rbind(train_una_clase, train.resto)
  # Asignamos como etiquetas 1 si pertenece a la clase considerada y 0 si no.
  train.CLASS.OVA = c(rep(1, n_datos_clase), rep(0, n_datos_clase))
  
  # Consideramos como test el resto de datos de train
  test.OVA <- train10[-c(train_idx, which(CLASS.train == clase)), ]
  
  # Hacemos cross validation para el problema binomial
  cv.fit <- cv.glmnet(as.matrix(train.OVA), train.CLASS.OVA, alpha = 0, 
                      family = "binomial", type.measure = "mse")
  lambda <- cv.fit$lambda.min
  # Entrenamos un modelo binomial
  modelo <- glmnet(as.matrix(train.OVA), train.CLASS.OVA, alpha = 0, 
                   lambda = lambda, family="binomial")
  predicciones <- predict(modelo, s=lambda, newx = test.OVA, type = "response")
  data <- as.matrix(data.frame(predicciones))
  
  clases.predichas = rep(0, nrow(test.OVA))
  clases.predichas[predicciones > .5] = 1
  
  # Obtenemos el porcentaje de acierto
  cat("Porcentaje de acierto para la clase", i, ":", 
      100*sum(clases.predichas == 0)/nrow(test.OVA), "\n")
}
```

Como vemos devuelve cosas lógicas en tanto que los porcentajes de acierto son relativamente altos, con lo que el problema no es que las etiquetas no tengan relación con los datos. Lo que vamos a hacer entonces es programar para este caso nosotros la clasificación multiclase haciendo un OVA con regresión logística binaria utilizando para la binaria el paquete `glmnet`.

```{r}
generarOVA <- function(train, clase, clases) {
  # Asignamos como etiquetas 1 si pertenece a la clase considerada y 0 si no.
  train.CLASS.OVA <- clases
  train.CLASS.OVA[clases == clase] <- 1
  train.CLASS.OVA[clases != clase] <- 0
  
  # Hacemos cross validation para el problema binomial
  cv.fit <- cv.glmnet(as.matrix(train), train.CLASS.OVA, alpha = 0, 
                      family = "binomial", type.measure = "mse")
  lambda <- cv.fit$lambda.min
  # Entrenamos un modelo binomial
  modelo <- glmnet(as.matrix(train), train.CLASS.OVA, alpha = 0, 
                   lambda = lambda, family="binomial")
  return(list(modelo, lambda))
}

predictOVA <- function(train, clases, numClases, test) {
  prediccionesOVA <- matrix(0, nrow(test), numClases)
  for (i in 1:numClases) {
    Mi <- generarOVA(train, i, clases)
    modeloi <- Mi[[1]]
    lambdai <- Mi[[2]]
    prediccionesOVA[,i] <- predict(modeloi, s=lambdai, newx = test, 
                                   type = "response")
  }
  
  clases.predichas <- apply(prediccionesOVA, 1, which.max)
  return(clases.predichas)
}
```








NO PARAMÉTRICO:

Para elegir entre KNN y RBF (funciones de base radial) vamos a volver a utilizar el índice *J* que habíamos calculado previamente para elegir entre regresión logística y SVM, ya que aquí también nos influye cómo de solapadas estén las clases, ya que si no hay solapamiento convendría más utilizar KNN ya que ahí los K vecinos más cercanos influyen lo mismo para elegir la clase, mientras que si hay solapamiento conviene más utilizar funciones de base radial para que influyan menos los valores que están más lejanos, ya que al tenerlos mezclados en la frontera, a un punto lo suficientemente cercano a la frontera le viene mejor que estos influyan menos a que los coja por igual el KNN. Además, al estar las clases solapadas usando KNN corremos el riesgo de que los K vecinos más cercanos no sean (en mayoría) de la clase del punto a clasificar. Este problema lo evitamos empleando base radial puesto que en ella todos los puntos tienen influencía, así por ejemplo pueden vencer puntos que estén algo más alejados del punto en cuestión pero que sean de la clase correcta.  

Además de utilizar este índice, vamos a calcular la distancia media entre todos los puntos de una clase y la distancia media entre todos los puntos de la muestra, de forma que si la distancia media de una clase con respecto a la distancia media de la muestra total es igual o mayor, esto podría decirnos que hay solapamiento, mientras que si es menor, que no lo hay. Lo vamos a hacer tanto para NSP, donde tenemos 3 clases, como para CLASS, donde tenemos 10.    

```{r}
getDistanciasMedias <- function(clases, train, numClases) {
  distanciasMedias <- sapply(1:numClases, function(i) {
    if (is.null(clases)) {
      data <- as.matrix(train)
    }
    else {
      data <- which(clases == i)
      data <- as.matrix(train[data,])
    }
    distancia <- rdist(data)
    distMedia <- sum(distancia)/
      (nrow(distancia)*ncol(distancia)- nrow(distancia))
    distMedia
  })
  return(distanciasMedias)
}

```

```{r}
cat("Distancias medias de las 3 clases:", 
      getDistanciasMedias(NSP.train, train, 3))
cat("Distancia media de la muestra completa para 3 clases:",
    getDistanciasMedias(NULL, train, 1))
cat("Distancias medias de las 10 clases: ",
    getDistanciasMedias(CLASS.train, train10, 10))
cat("Distancia media de la muestra completa para 10 clases:",
    getDistanciasMedias(NULL, train10, 1))
cat("Distancias medias de las 3 clases con PCA:", 
      getDistanciasMedias(NSP.train, trainPCA, 3))
cat("Distancia media de la muestra completa para 3 clases con PCA:",
    getDistanciasMedias(NULL, trainPCA, 1))
cat("Distancias medias de las 10 clases con PCA: ",
    getDistanciasMedias(CLASS.train, trainPCA10, 10))
cat("Distancia media de la muestra completa para 10 clases con PCA:",
    getDistanciasMedias(NULL, trainPCA10, 1))
```

Efectivamente nos sale algo parecido a lo que deducíamos del índice *J*, ya que para las 3 clases (tanto con PCA como sin él), las medias de las clases están muy cerca o por encima de la media de la muestra total, mientras que para 10 clases (tanto con PCA como sin él), las medias están cerca de la media total de la muestra, con lo que tenemos más solapamiento con 3 clases que con 10 pero en ningún caso están completamente separadas.  

Vamos a utilizar por tanto funciones de base radial. A continuación vemos la implementación de las mismas, que hemos llevado a cabo nosotros. 

Vamos a hacer primero una función para hacer la clasificación con las funciones de base radial, donde en lugar de hacer la media de las distancias con las etiquetas, lo que vamos a hacer es sumar pesos que irán en función de *r* y el punto de test en cuestión. Donde *r* es la anchura que le vamos a dar a las gaussianas, que va a ser un valor que vamos a obtener por validación cruzada más adelante.

```{r}
#FUNCIONES DE BASE RADIAL

#Función que devuelve un núcleo Gaussiano normalizado para R^d
fiD <- function(d){
  function(z) {exp(-0.5 * z^2)/((2*pi)^(-d/2))}
}

distancia <- function(x, y) {
  sqrt(sum((x-y)^2))
}

RBF <- function(x, datos.train, et.train, numClases, r) {
  fi <- fiD(ncol(datos.train))
  alfas <- apply(datos.train, 1, function(y) { fi(distancia(x,y)/r) } )
  suma_total  <- sum(alfas)
  sumas_parciales <- sapply(1:numClases, function(i) {
    sum(alfas[et.train == i])
  }) 
  sumas_parciales <- sumas_parciales/suma_total
  which.max(sumas_parciales)
}
```

```{r}
predictEt <- function(datos.train, et.train, datos.test, numClases, r) {
  etiquetas <- sapply(1:nrow(datos.test), function(i) {
    RBF(datos.test[i,], datos.train, et.train, numClases, r)
  })
  return(etiquetas)
}
```

```{r}
cv.RBF <- function(datos.train, et.train, numClases, rango_r) {
  # Realizamos las particiones de forma equilibrada
  k = 5
  folds = rep(1, nrow(datos.train))
  for (i in 1:numClases) {
    folds[et.train == i] <- c(sample(rep(seq(k), floor(sum(et.train == i)/k))),
                              rep(1, sum(et.train == i) - 
                                    k*floor(sum(et.train == i)/k)))
  }
  
  media_aciertos <- vector("numeric", length(rango_r))
  
  for(j in 1:length(rango_r)) {
    cat("El r a probar es: ", rango_r[j], "\n")
    aciertos <- vector("numeric", k)
    for(i in 1:k) {
      cat("Fold: ", i, "\n")
      etiq <- unlist(predictEt(datos.train[folds != i, ], et.train[folds != i], 
                datos.train[folds == i, ], numClases, rango_r[j]))
      print(etiq)
      aciertos[i] <- sum(etiq == et.train[folds == i])/length(etiq)
      cat("aciertos[",i,"] =", aciertos[i], "\n")
    }
    print(aciertos)
    media_aciertos[j] <- mean(aciertos)
    print(media_aciertos[j])
  }
  
  return(rango_r[which.max(media_aciertos)])
  
}
```

 




#7. Argumentar sobre la idoneidad de la función regularización usada (en su caso).


#8. Valoración de los resultados (gráficas, métricas de error, análisis de residuos, etc.)


#9. Justificar que se ha obtenido la mejor de las posibles soluciones con la técnica elegida y la muestra dada. Argumentar en términos de la dimensión VC del modelo, el error de generalización y las curvas de aprendizaje.



# Bibliografía

\begin{enumerate}
\item La base de datos: \url{https://archive.ics.uci.edu/ml/datasets/Cardiotocography#}
\item PCA con `R`: \url{http://www.r-bloggers.com/computing-and-visualizing-pca-in-r/}
\item Partición de los datos: \href{http://stackoverflow.com/questions/13536537/partitioning-data-set-in-r-based-on-multiple-classes-of-observations}{http://stackoverflow.com/questions/...}
\item \url{http://www.prasa.org/proceedings/2004/prasa04-12.pdf}
\item \url{http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1791&context=eispapers}
\end{enumerate}